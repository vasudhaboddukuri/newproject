{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab's New Code Editor",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudhaboddukuri/newproject/blob/master/Colab's_New_Code_Editor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwHQcveBaZIt",
        "colab_type": "text"
      },
      "source": [
        " **Mounting Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrpk8T4VWw_T",
        "colab_type": "code",
        "outputId": "f50307a0-d47b-412b-b561-36572b418834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct6HwzU-anJb",
        "colab_type": "text"
      },
      "source": [
        " **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ukfaQnwaP9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ov-NUicaSgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctr = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2glcUmdbHv1",
        "colab_type": "text"
      },
      "source": [
        "### **Routines for cleaning the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Gq0QkNazCK",
        "colab_type": "text"
      },
      "source": [
        " **1. Remove all non-ascii characters**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_hSXoxZbCjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_nonascii(text):\n",
        "    return text.encode(\"ascii\",errors=\"ignore\").decode()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPOd4ufhbYVy",
        "colab_type": "text"
      },
      "source": [
        "**2. Remove html(if any):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4bw-ycgbFWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_html(text):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', text)\n",
        "    return cleantext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qFiRHEBbtRs",
        "colab_type": "text"
      },
      "source": [
        "**3. Replace different punctuation with whitespace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtEKiXcybmxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_punc(text):\n",
        "    return (text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti39K8rtb5GW",
        "colab_type": "text"
      },
      "source": [
        "**4. Make all lowercase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rae-chByb2Me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_case(text):\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvYwATS3cCRZ",
        "colab_type": "text"
      },
      "source": [
        "**5. Clear extra whitespaces**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcwPHDi3cBP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_ws(text):\n",
        "    return (' '.join(text.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWK5_bHcSBu",
        "colab_type": "text"
      },
      "source": [
        "**6. Clear stop_words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju-leEHpcRbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_stopwords(text):\n",
        "    setstopwords = set(stopwords.words('english'))\n",
        "    return \" \".join(word for word in text.split() if word not in setstopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08muvKtUchR0",
        "colab_type": "text"
      },
      "source": [
        "**7. Part of speech - helper function for lemmatization - to ensure verb and noun\n",
        "forms are treated differently**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lic0p8ChcgGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\":wordnet.ADJ,\n",
        "                \"N\":wordnet.NOUN,\n",
        "                \"V\":wordnet.VERB,\n",
        "                \"R\":wordnet.ADV}\n",
        "    #default is noun\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2f6_WccyPZ",
        "colab_type": "text"
      },
      "source": [
        "**8. Lemmatize all sentences basis POS tag**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHpDUc3-cxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dolemma(text):\n",
        "    global ctr\n",
        "    print(ctr)\n",
        "    ctr = ctr + 1\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_list = nltk.word_tokenize(text)\n",
        "    lemmatized_output = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_list])\n",
        "    return lemmatized_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVDfIdiec6nu",
        "colab_type": "text"
      },
      "source": [
        "**Combined Clean Routine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIaPejaBc5Ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleandf(df):\n",
        "    \n",
        "    \n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_case(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_nonascii(str(row['Text'])), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_html(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_punc(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_dig(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_ws(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: clean_stopwords(row['Text']), axis=1)\n",
        "    df['Text'] = df[['Text']].apply(lambda row: dolemma(row['Text']), axis=1)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSloxbitdaMR",
        "colab_type": "text"
      },
      "source": [
        "**Load the file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sna9HCTldB8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(r\"/content/drive/My Drive/Data_Crawling/Iraq_War_Data.csv\",encoding = \"ISO-8859-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}